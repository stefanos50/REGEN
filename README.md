

# REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework

<div align="center">
  <img src="https://drive.google.com/thumbnail?id=1W8nu1exktsaBzE0c7pKs5Qc709uz67VO&sz=w1000" alt="Image" width="400px" height="auto">
  <img src="https://drive.google.com/thumbnail?id=1CCytLrJvEtahGwy8Ae0KCx4v5tL3Gi5b&sz=w1000" alt="Image" width="400px" height="auto">
</div>

## Abstract

Photorealism is an important aspect of modern video games since it can shape the player experience and simultaneously impact the immersion, narrative engagement, and visual fidelity. Although recent hardware technological breakthroughs, along with state-of-the-art rendering technologies, have significantly improved the visual realism of video games, achieving true photorealism in dynamic environments at real-time frame rates still remains a major challenge due to the tradeoff between visual quality and performance. In this short paper, we present a novel approach for enhancing the photorealism of rendered game frames using generative adversarial networks. To this end, we propose Real-time photorealism Enhancement in Games via a dual-stage gEnerative Network framework (REGEN), which employs a robust unpaired image-to-image translation model to produce semantically consistent photorealistic frames that transform the problem into a simpler paired Im2Im translation task. This enables the training with a lightweight method that can achieve real-time inference time without compromising visual quality. We demonstrate the effectiveness of our method on Grand Theft Auto V, showing that the approach achieves visual results comparable to the ones produced by the robust unpaired Im2Im method while improving inference speed by 32.14x times. Our findings also indicate that the results outperform the photorealism-enhanced frames produced by directly training a lightweight unpaired Im2Im translation method to translate the video game frames towards the visual characteristics of real-world images.

### BibTeX Citation

If you used the REGEN framwork or any of the pretrained models from this repository in a scientific publication, we would appreciate using the following citation:

```
@ARTICLE{11122908,
  author={Pasios, Stefanos},
}
```

> 📝 **Note**: This repository uses code from the Pix2PixHD repository that can be found [here](https://github.com/NVIDIA/pix2pixHD).

## Training

To train the model, it is required to have access to a synthetic dataset generated by a game or simulator and the corresponding images that were photorealism enhanced by a robust unpaired image-to-image translation method such as [Enhancing Photorealism Enhancement (EPE)](https://github.com/isl-org/PhotorealismEnhancement). 

### CARLA Simulator

To train a model that enhances the photorealism of the CARLA simulator towards the characteristics of real-world datasets (Mapillary Vistas, Cityscapes, and KITTI), we already provide both the original rendered frames and the results of EPE [here](https://www.kaggle.com/datasets/stefanospasios/carla2real-enhancing-the-photorealism-of-carla).

### Grand Theft Auto V

To train a model that enhances the photorealism of GTAV towards the characteristics of real-world datasets (Mapillary Vistas and Cityscapes), the results of EPE are already provided by the authors at the [official repository](https://github.com/isl-org/PhotorealismEnhancement). The initial rendered GTAV frames originated from the Playing for Data dataset, which can be downloaded [here](https://download.visinf.tu-darmstadt.de/data/from_games/).

### Starting the Training

After collecting the required datasets, place the training and test sets of the game/simulator dataset into `./data/train_A` and `./data/test_A`, respectively. The corresponding photorealism-enhanced images should be transferred into the `./data/train_B` and `./data/test_B` directories. To start training, execute the following command:

```javascript
python train.py --dataroot ./data --name REGEN --label_nc 0 --no_instance --gpu_id 0
```

## Testing

To test the framework, we provide pretrained models for `GTAV → Cityscapes`, `CARLA → Cityscapes`, and `CARLA → KITTI`. Download the models from [Google Drive](https://drive.google.com/drive/folders/19Q8E9wy3MR-vUfOfVwytIzv4QNpndYo0?usp=sharing) and transfer them into `./checkpoints/REGEN/`. Finally, transfer the images that are to be inferred with the model in the `./data/test_A` directory and execute the following command:

```javascript
python test.py --dataroot ./data --name REGEN --label_nc 0 --no_instance --gpu_id 0
```

The resulting images will be saved in the `./results/REGEN/images/` directory.

## Real-Time Inference

We additionally provide two sample scripts for testing the models in real-time conditions. The provided pretrained models should be placed in the same directory as for testing.

### CARLA Simulator

To test the model on CARLA, download the UE4 executable of the simulator from the [official repository](https://github.com/carla-simulator/carla/releases). Particularly, the code was tested with CARLA version 0.9.15. After running the simulator and initializing the world, execute the following command:

```javascript
python carla_test.py --dataroot ./data --name REGEN --label_nc 0 --no_instance --gpu_id 0
```

### Grand Theft Auto V

To test the model on GTA V, download the game and set it to the minimum graphics settings. Considering that the script performs real-time capturing of the game window, run the game in windowed mode with a lower resolution of the monitor. Then execute the following script:

```javascript
python gta_test.py --dataroot ./data --name REGEN --label_nc 0 --no_instance --gpu_id 0
```

> 📝 **Note**: All the available parameters of the model (e.g., for changing the resolution of the resulting images) can be found in `./options/`.


